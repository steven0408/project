{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Gobang as go\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 100)\n",
    "        self.fc2 = nn.Linear(100, 64)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "eval_model = Net()\n",
    "eval_optimizer = optim.RMSprop(eval_model.parameters(), lr=1e-3)\n",
    "target_model = Net()\n",
    "target_optimizer = optim.RMSprop(target_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.buffer = torch.zeros(500, 8*8*2*2 + 2)\n",
    "        self.buffer_counter = 0\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 1e-2\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_increment = 1e-4\n",
    "        self.epsilon_max = .9\n",
    "        self.replace_target_iter = 50   # 一定的時候替換神經網路參數\n",
    "        self.learn_step_counter = 0\n",
    "        if os.path.isfile('eval_record.pt'):\n",
    "            checkpoint = torch.load('eval_record.pt')\n",
    "            eval_model.state_dict(checkpoint['model_state_dict'])\n",
    "            eval_model.train()\n",
    "        if os.path.isfile('target_record.pt'):\n",
    "            checkpoint = torch.load('target_record.pt')\n",
    "            target_model.state_dict(checkpoint['model_state_dict'])\n",
    "            target_model.train()\n",
    "        \n",
    "    # 將 8 * 8 棋盤轉換成 1 * 128 的 tensor 形式\n",
    "    def board_transform(self, Board):\n",
    "        black_board = np.zeros((1, 64))\n",
    "        white_board = np.zeros((1, 64))\n",
    "        flatten_board = np.reshape(Board, [1, 64])\n",
    "        b = np.where(flatten_board==1)[1]\n",
    "        w = np.where(flatten_board==-1)[1]\n",
    "        black_board[0, b] = 1\n",
    "        white_board[0, w] = 1\n",
    "        s = np.hstack((black_board, white_board))\n",
    "        s = torch.FloatTensor(s)\n",
    "        return s\n",
    "\n",
    "    def play(self, Board, chessman):\n",
    "        step = 0\n",
    "        for eposide in range(10):\n",
    "            board = Board.copy()\n",
    "            chess = chessman\n",
    "            while go.GetValid(board) != []:\n",
    "                action = self.choose_action(board)\n",
    "                observation = self.board_transform(board)\n",
    "                board[int(action/8), int(action%8)] = chessman\n",
    "                next_observation = self.board_transform(board)\n",
    "                reward = self.reward_rule(board, [int(action/8), int(action%8)])\n",
    "                self.store_transition(observation, action, reward, next_observation)\n",
    "                chess = -chess\n",
    "                if step > 200 and step % 100 == 0:\n",
    "                    self.training()\n",
    "                step += 1\n",
    "                if go.IsContinuous(board, [int(action/8), int(action%8)]):\n",
    "                    break\n",
    "\n",
    "    \n",
    "    def store_transition(self, state, action, reward, new_state):\n",
    "        action = np.reshape(action, [1, 1])\n",
    "        reward = np.reshape(reward, [1, 1])\n",
    "        state = np.reshape(state, [1, 8*8*2])\n",
    "        new_state = np.reshape(new_state, [1, 8*8*2])\n",
    "        transition = np.hstack((state, action, reward, new_state))\n",
    "        transition = torch.FloatTensor(transition)\n",
    "\n",
    "        index = self.buffer_counter % 500   # buffer_size = 500\n",
    "        self.buffer[index, :] = transition\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    def choose_action(self, Board):\n",
    "        # 選擇最佳解\n",
    "        if np.random.uniform() <= self.epsilon:\n",
    "            observation = self.board_transform(Board)\n",
    "            action_value = eval_model(observation)\n",
    "            max_value = float('-inf')\n",
    "            max_action = -1\n",
    "            for x, y in go.GetValid(Board):\n",
    "                if action_value[0, 8*x+y].item() > max_value:\n",
    "                    max_value = action_value[0, 8*x+y].item()\n",
    "                    max_action = 8 * x + y\n",
    "            return max_action\n",
    "        # 選擇隨機點\n",
    "        else:\n",
    "            x, y = random.choice(go.GetValid(Board))\n",
    "            return 8 * x + y  \n",
    "\n",
    "    def reward_rule(self, Board, action):\n",
    "        return 1 if go.IsContinuous(Board, action) else 0\n",
    "\n",
    "    def training(self):\n",
    "        # 檢查是否替換 target_net 參數\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.replace_target_params()\n",
    "        if self.buffer_counter > 500:    # buffer_size = 500\n",
    "            sample_index = np.random.choice(500, self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.buffer_counter, self.batch_size)\n",
    "        batch_buffer = self.buffer[sample_index, :]\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        print(self.buffer)\n",
    "        state = batch_buffer[:, -128:]\n",
    "        next_state = batch_buffer[:, :128]\n",
    "        Q_eval = target_model(state)\n",
    "        Q_next = eval_model(next_state)\n",
    "        Q_target = torch.clone(Q_eval)\n",
    "        print(batch_buffer[:, 128])\n",
    "        eval_act_index = batch_buffer[:, 128]\n",
    "\n",
    "        reward = batch_buffer[:, 128 + 1]\n",
    "        Q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(Q_next, axis=1)\n",
    "        loss = nn.MSELoss(Q_target, Q_eval)\n",
    "        eval_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        eval_optimizer.step()\n",
    "    \n",
    "    def replace_target_params(self):\n",
    "        for p, q in zip(eval_model.parameters(), target_model.parameters()):\n",
    "            q.data = p.data\n",
    "        \n",
    "    def move(self, Board, action, chessman):\n",
    "        self.play(Board, chessman)\n",
    "        action = self.choose_action(Board)\n",
    "        torch.save({'model_state_dict': eval_model.state_dict()}, 'eval_record.pt')\n",
    "        torch.save({'model_state_dict': target_model.state_dict()}, 'target_record.pt')\n",
    "        return [int(action/8), int(action%8)]\n",
    "\n",
    "    def choose(self, Board, action, chessman):\n",
    "        return random.choice([-1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "Board = np.zeros((8,8))\n",
    "Board[4] = [0., 0., 0., 1., 1., -1, 0. ,0.]\n",
    "print(Board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([30., 49., 12., 19., 59., 51.,  0., 12.,  6.,  9., 14., 33., 56.,  9.,\n",
      "        50., 47., 50., 47.,  0., 54.,  7., 21.,  6., 43., 43., 23.,  5., 14.,\n",
      "         9., 48., 48., 19., 46., 63.,  8., 11., 19., 22., 38., 33., 61.,  8.,\n",
      "        29.,  6.,  1., 29., 45., 29., 13., 51., 46., 48., 61., 21., 27., 50.,\n",
      "         3., 32.,  5., 16., 56.,  7., 56.,  7.])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16712/3140861785.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBoard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchessman\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16712/301317487.py\u001b[0m in \u001b[0;36mmove\u001b[1;34m(self, Board, action, chessman)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBoard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchessman\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBoard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchessman\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBoard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'model_state_dict'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'eval_record.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16712/301317487.py\u001b[0m in \u001b[0;36mplay\u001b[1;34m(self, Board, chessman)\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mchess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mchess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsContinuous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16712/301317487.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mQ_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0meval_act_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "a = DQN()\n",
    "a.move(Board, [4,3], chessman=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(check)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "582296f84aac35676d48f873b0016f7ea59eaa42b87f7345a0b548322594b08f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
